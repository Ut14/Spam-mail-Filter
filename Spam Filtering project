# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from sklearn.model_selection import GridSearchCV

# Load your dataset
# For simplicity, let's assume you have a CSV file named 'spam_dataset.csv'
# with columns like 'text' and 'label' where 'label' is 0 for non-spam and 1 for spam
spam_data = pd.read_csv('spam_dataset.csv')

# Text preprocessing using NLTK
stop_words = set(stopwords.words('english'))
ps = PorterStemmer()

def preprocess_text(text):
    words = word_tokenize(text)
    words = [ps.stem(word) for word in words if word.isalnum() and word.lower() not in stop_words]
    return ' '.join(words)

spam_data['processed_text'] = spam_data['text'].apply(preprocess_text)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    spam_data['processed_text'], spam_data['label'], test_size=0.2, random_state=42
)

# Create a pipeline with text processing and the classifier (Multinomial Naive Bayes in this case)
pipeline = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', MultinomialNB())
])

# Define hyperparameters for grid search
param_grid = {
    'vect__ngram_range': [(1, 1), (1, 2)],
    'tfidf__use_idf': [True, False],
    'clf__alpha': [1e-2, 1e-3, 1e-4]
}

# Perform grid search for model tuning
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Print the best parameters from the grid search
print("Best parameters found: ", grid_search.best_params_)

# Make predictions on the test set using the best model
best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, predictions)
print(f'Accuracy on Test Set: {accuracy}')
print('Classification Report:\n', classification_report(y_test, predictions))
print('Confusion Matrix:\n', confusion_matrix(y_test, predictions))

# Now, you can use the best model to predict whether a new email is spam or not
new_email = ["Congratulations! You've won a free vacation. Click here to claim your prize."]
new_email_processed = preprocess_text(new_email[0])
new_prediction = best_model.predict([new_email_processed])
print(f'Predicted Label for New Email: {new_prediction[0]}')
